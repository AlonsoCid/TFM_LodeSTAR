{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask overstimator comparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom dataset class to make it compatible with the DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# dataset1\n",
    "dataset = torch.load('data/dataset_C.pt')\n",
    "\n",
    "train_dataset_C = dataset['train']\n",
    "test_dataset_C = dataset['test']\n",
    "\n",
    "train_dataloader_C = DataLoader(train_dataset_C, batch_size=8, shuffle=True)\n",
    "test_dataloader_C = DataLoader(test_dataset_C, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset2\n",
    "dataset = torch.load('data/dataset_CE.pt')\n",
    "\n",
    "train_dataset_CE = dataset['train']\n",
    "test_dataset_CE = dataset['test']\n",
    "\n",
    "train_dataloader_CE = DataLoader(train_dataset_CE, batch_size=8, shuffle=True)\n",
    "test_dataloader_CE = DataLoader(test_dataset_CE, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LodeSTAR2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "        dropout = nn.Dropout2d(p=0.005)\n",
    "        mask_gumbel = dropout(mask_gumbel)\n",
    "        \n",
    "        weights =  mask_gumbel\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "        \n",
    "        detections = [row[::-1] for row in detections]\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py:454: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1eb7569394042b2e33e97808bdda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarDa = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "trainer_lodestarDa = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarDa.fit(lodestarDa, train_dataloader_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddf956b16414fb48476bccf56532739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD2a = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestarD2a = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD2a.fit(lodestarD2a, train_dataloader_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsDa = []\n",
    "for batch in test_dataloader_C:\n",
    "    data, *_ = batch\n",
    "    output = lodestarDa((data)).detach()\n",
    "    outputsDa.append(output)\n",
    "\n",
    "outputsDa = torch.cat(outputsDa, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.2737\n",
      "True positives %: 100%\n",
      "False positives: 0.11\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_C, outputsDa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD2a = []\n",
    "for batch in test_dataloader_CE:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD2a((data)).detach()\n",
    "    outputsD2a.append(output)\n",
    "\n",
    "outputsD2a = torch.cat(outputsD2a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.0881\n",
      "True positives %: 70%\n",
      "False positives: 0.31\n",
      "Class 2:\n",
      "Jaccard Index: 0.3455\n",
      "True positives %: 96%\n",
      "False positives: 0.03\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_CE, outputsD2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LodeSTAR 2.5 no mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "        dropout = nn.Dropout2d(p=0.005)\n",
    "        mask_gumbel = dropout(mask_gumbel)\n",
    "        \n",
    "        weights =  mask_gumbel\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 5*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "        \n",
    "        detections = [row[::-1] for row in detections]\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879a8eac6a14469bbb8eff4cd6f1f809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarDb = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "trainer_lodestarDb = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarDb.fit(lodestarDb, train_dataloader_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2cdabefbff4173a637df8b15ddb189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD2b = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestarD2b = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD2b.fit(lodestarD2b, train_dataloader_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsDb = []\n",
    "for batch in test_dataloader_C:\n",
    "    data, *_ = batch\n",
    "    output = lodestarDb((data)).detach()\n",
    "    outputsDb.append(output)\n",
    "\n",
    "outputsDb = torch.cat(outputsDb, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.3364\n",
      "True positives %: 100%\n",
      "False positives: 0.0\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_C, outputsDb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD2b = []\n",
    "for batch in test_dataloader_CE:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD2b((data)).detach()\n",
    "    outputsD2b.append(output)\n",
    "\n",
    "outputsD2b = torch.cat(outputsD2b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.1671\n",
      "True positives %: 84%\n",
      "False positives: 0.18\n",
      "Class 2:\n",
      "Jaccard Index: 0.5060\n",
      "True positives %: 99%\n",
      "False positives: 0.0\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_CE, outputsD2b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
