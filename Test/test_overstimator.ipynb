{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask overstimator comparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom dataset class to make it compatible with the DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# dataset1\n",
    "dataset = torch.load('dataset1.pt')\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset2\n",
    "dataset2 = torch.load('dataset2.pt')\n",
    "\n",
    "train_dataset2 = dataset2['train']\n",
    "test_dataset2 = dataset2['test']\n",
    "\n",
    "train_dataloader2 = DataLoader(train_dataset2, batch_size=8, shuffle=True)\n",
    "test_dataloader2 = DataLoader(test_dataset2, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset3\n",
    "dataset3 = torch.load('dataset3s.pt')\n",
    "\n",
    "train_dataset3 = dataset3['train']\n",
    "test_dataset3 = dataset3['test']\n",
    "\n",
    "train_dataloader3 = DataLoader(train_dataset3, batch_size=8, shuffle=True)\n",
    "test_dataloader3 = DataLoader(test_dataset3, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset4\n",
    "test_datasetM = torch.load('dense_dataset.pt')\n",
    "\n",
    "test_dataloaderM = DataLoader(test_datasetM, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigmask-mask method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Callable, Optional\n",
    "\n",
    "# import numpy as np\n",
    "# import scipy\n",
    "# import scipy.ndimage\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from skimage import morphology\n",
    "\n",
    "# from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "# from deeplay.applications.detection.lodestar.transforms import (\n",
    "#     RandomRotation2d,\n",
    "#     RandomTranslation2d,\n",
    "#     Transforms,\n",
    "# )\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class LodeSTAR(Application):\n",
    "\n",
    "#     # num_outputs: int # only 2D for now\n",
    "#     num_classes: int\n",
    "#     transforms: Transforms\n",
    "#     n_transforms: int\n",
    "#     model: nn.Module\n",
    "#     between_loss: Callable\n",
    "#     within_loss: Callable\n",
    "#     between_loss_weight: float\n",
    "#     within_loss_weight: float\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model: Optional[nn.Module] = None,\n",
    "#         # num_outputs: int = 2,\n",
    "#         num_classes: int = 2,\n",
    "#         transforms: Optional[Transforms] = None,\n",
    "#         n_transforms: int = 2,\n",
    "#         between_loss: Optional[Callable] = None,\n",
    "#         within_loss: Optional[Callable] = None,\n",
    "#         between_loss_weight: float = 1,\n",
    "#         within_loss_weight: float = 10,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         if transforms is None:\n",
    "#             transforms = Transforms(\n",
    "#                 [\n",
    "#                     RandomTranslation2d(),\n",
    "#                     RandomRotation2d(),\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#         self.num_classes = num_classes\n",
    "#         self.transforms = transforms\n",
    "#         self.n_transforms = n_transforms\n",
    "#         self.model = model or self._get_default_model()\n",
    "#         self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "#         self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "#         self.between_loss_weight = between_loss_weight\n",
    "#         self.within_loss_weight = within_loss_weight\n",
    "\n",
    "#         super().__init__(loss=None, **kwargs)\n",
    "\n",
    "#     def _get_default_model(self):\n",
    "#         cnn = ConvolutionalNeuralNetwork(\n",
    "#             None,\n",
    "#             [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "#             (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "#         )\n",
    "#         cnn.blocks[2].pooled()\n",
    "\n",
    "#         return cnn\n",
    "\n",
    "#     def transform_data(self, batch):\n",
    "#         repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "#         transformed, inverse = self.transforms(repeated)\n",
    "#         return transformed, inverse\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         if self.training:\n",
    "#             x, class_label = x\n",
    "\n",
    "#         # else:\n",
    "#         #     x, _, _ = x\n",
    "\n",
    "#         out = self.model(x)\n",
    "#         y = out[:, :3, ...]\n",
    "#         classes=out[:, 3:, ...]\n",
    "#         classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "#         batch_size = classes.size(0)\n",
    "#         num_channels = classes.size(1)\n",
    "#         _, _, Hx, Wx = x.shape\n",
    "#         _, _, Hy, Wy = y.shape\n",
    "#         x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "#         y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "#         if self.training:\n",
    "#             x_range = x_range - Hx / 2 + 0.5\n",
    "#             y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "#             batch_indices = torch.arange(batch_size)\n",
    "\n",
    "#             mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "#         else:\n",
    "            \n",
    "#             mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "#         Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "#         delta_x = y[:, 0:1, ...]\n",
    "#         delta_y = y[:, 1:2, ...]\n",
    "#         yy = y[:, 2:3, ...]\n",
    "#         weights = y[:, 2:3, ...].sigmoid()\n",
    "#         X = X + delta_x\n",
    "#         Y = Y + delta_y\n",
    "        \n",
    "#         return torch.cat(\n",
    "#             [X, Y, weights, mask, classes], dim=1\n",
    "#         )\n",
    "\n",
    "#     def normalize(self, weights):\n",
    "#         weights = weights + 1e-5\n",
    "#         return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "#     def reduce(self, X, weights):\n",
    "#         return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "#     def compute_loss(self, y_hat, inverse_fn):\n",
    "#         B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "#         y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "#         weights =  mask_gumbel\n",
    "#         weights = self.normalize(weights)\n",
    "#         y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "#         within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "#         within_disagreement_loss = self.within_loss(\n",
    "#             within_disagreement, torch.zeros_like(within_disagreement)\n",
    "#         )\n",
    "\n",
    "#         y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "#         between_disagreement_loss = 0\n",
    "\n",
    "#         for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "#             batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "#             batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "#                 batch_preds\n",
    "#             )\n",
    "#             between_disagreement_loss += (\n",
    "#                 self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "#             )\n",
    "#         weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "#         weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "#         compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "#         mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        # Overestimation loss\n",
    "        kernel = torch.ones((1, 1, 3, 3), device=y_hat.device)\n",
    "        dilated_mask = F.conv2d(mask_gumbel, kernel, padding=1) > 0\n",
    "        overestimation_loss = 0.015*(dilated_mask.float() - mask_gumbel).mean()\n",
    "\n",
    "#         return {\n",
    "#             \"between_image_disagreement\": weighted_between_loss,\n",
    "#             \"within_image_disagreement\": weighted_within_loss,\n",
    "#             \"mask_loss\": mask_loss,\n",
    "#             \"overestimation_loss\": overestimation_loss,\n",
    "#         }\n",
    "\n",
    "#     def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "#         \"\"\"Detects objects in a batch of images\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         x : array-like\n",
    "#             Input to model\n",
    "#         alpha, beta: float\n",
    "#             Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "#         cutoff: float\n",
    "#             Threshold for detection\n",
    "#         mode: string\n",
    "#             Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "#             `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "#             score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "#         \"\"\"\n",
    "#         y = self(x.to(self.device))\n",
    "#         y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "#         detections = [\n",
    "#             self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "#             for i in range(len(y_pred))\n",
    "#         ]\n",
    "#         detections = [row[::-1] for row in detections]\n",
    "#         return detections\n",
    "\n",
    "#     def pooled(self, x, mask=1):\n",
    "#         \"\"\"Pooled output from model.\n",
    "\n",
    "#         Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "#         Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         x : array-like\n",
    "#             Input to model\n",
    "#         mask : array-like\n",
    "#             Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "#         \"\"\"\n",
    "#         y = self(x.to(self.device))\n",
    "#         y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "#         masked_weights = weights * mask\n",
    "\n",
    "#         pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "#         return pooled\n",
    "\n",
    "#     def detect_single(\n",
    "#         self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "#     ):\n",
    "#         \"\"\"Detects objects in a single image\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         y_pred, weights: array-like\n",
    "#             Output from model\n",
    "#         alpha, beta: float\n",
    "#             Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "#         cutoff: float\n",
    "#             Threshold for detection\n",
    "#         mode: string\n",
    "#             Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "#             `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "#             score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "#         \"\"\"\n",
    "#         score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "#         return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "#         \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "#         Parameters\n",
    "#             ----------\n",
    "#         pred, score: array-like\n",
    "#             Output from model, score-map\n",
    "#         cutoff, mode: float, string\n",
    "#             Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "#             `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "#             score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "#         \"\"\"\n",
    "#         score = score[3:-3, 3:-3]\n",
    "#         th = cutoff\n",
    "#         if mode == \"quantile\":\n",
    "#             th = np.quantile(score, cutoff)\n",
    "#         elif mode == \"ratio\":\n",
    "#             th = np.max(score.flatten()) * cutoff\n",
    "#         hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "#         hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "#         detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "#         return np.array(detections)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def local_consistency(pred):\n",
    "#         \"\"\"Calculate the consistency metric\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         pred : array-like\n",
    "#             first output from model\n",
    "#         \"\"\"\n",
    "#         pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "#         kernel = np.ones((3, 3, 1)) / 3**2\n",
    "#         pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "#         squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "#         squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "#         np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "#         return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "#     @classmethod\n",
    "#     def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "#         \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         pred, weights: array-like\n",
    "#             Output from model\n",
    "#         alpha, beta: float\n",
    "#             Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "#         \"\"\"\n",
    "#         return (\n",
    "#             weights[0].detach().cpu().numpy() ** alpha\n",
    "#             * cls.local_consistency(pred) ** beta\n",
    "#         )\n",
    "\n",
    "#     def train_preprocess(self, batch):\n",
    "#         batch, class_label = batch\n",
    "#         x, inverse = self.transform_data(batch)\n",
    "#         class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "#         return (x, class_label), inverse\n",
    "\n",
    "#     def val_preprocess(self, batch):\n",
    "#         batch,_,_ = batch\n",
    "#         x, inverse = self.transform_data(batch)\n",
    "#         return (x,), inverse\n",
    "\n",
    "#     test_preprocess = val_preprocess\n",
    "\n",
    "#     def on_train_end(self) -> None:\n",
    "#         self.eval()\n",
    "#         return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import numpy as np\n",
    "\n",
    "# lodestarB = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "# trainer_lodestarB = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "# trainer_lodestarB.fit(lodestarB, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lodestarB2 = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "# trainer_lodestarB2 = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "# trainer_lodestarB2.fit(lodestarB2, train_dataloader2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation tests\n",
    "Calculate the jaccard index and percentage of successful detected objects by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputsB = []\n",
    "# for batch in test_dataloader:\n",
    "#     data, *_ = batch\n",
    "#     output = lodestarB((data)).detach()\n",
    "#     outputsB.append(output)\n",
    "\n",
    "# outputsB = torch.cat(outputsB, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from segmentation_tests import segmentation_tests\n",
    "\n",
    "# segmentation_tests(test_dataset, outputsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputsB2 = []\n",
    "# for batch in test_dataloader2:\n",
    "#     data, *_ = batch\n",
    "#     output = lodestarB2((data)).detach()\n",
    "#     outputsB2.append(output)\n",
    "\n",
    "# outputsB2 = torch.cat(outputsB2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from segmentation_tests import segmentation_tests\n",
    "\n",
    "# segmentation_tests(test_dataset2, outputsB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For negative detection, check for a pixel and pixels right next to each other, in a way that we are able to identify individual chunks of 1, if those chunks don't overlap with the ground truth mask it's a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        # else:\n",
    "        #     x, _, _ = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "        dropout_rate = 0.05\n",
    "        dropout_mask = torch.bernoulli((1 - dropout_rate) * torch.ones_like(mask_gumbel))\n",
    "        mask_gumbel = mask_gumbel * dropout_mask\n",
    "\n",
    "        weights =  mask_gumbel\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "        \n",
    "        detections = [row[::-1] for row in detections]\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py:454: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52258b62fc34862b11ed1428a90ace3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "trainer_lodestarD = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD.fit(lodestarD, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef328fe3f210469f827f742b8d613f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD2 = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestarD2 = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD2.fit(lodestarD2, train_dataloader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py:454: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  253 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  253 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 253 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 253 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 253 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 253 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c415c8a2a15346b98041b401eb0c96fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (38) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD3 = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=3).build()\n",
    "trainer_lodestarD3 = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD3.fit(lodestarD3, train_dataloader3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation tests\n",
    "Calculate the jaccard index and percentage of successful detected objects by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD = []\n",
    "for batch in test_dataloader:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD((data)).detach()\n",
    "    outputsD.append(output)\n",
    "\n",
    "outputsD = torch.cat(outputsD, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.8604\n",
      "Percentage of matches in class 1: 100%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset, outputsD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD2 = []\n",
    "for batch in test_dataloader2:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD2((data)).detach()\n",
    "    outputsD2.append(output)\n",
    "\n",
    "outputsD2 = torch.cat(outputsD2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.8795\n",
      "Percentage of matches in class 1: 85%\n",
      "Jaccard Index for class 2: 0.7993\n",
      "Percentage of matches in class 2: 100%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset2, outputsD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD3 = []\n",
    "for batch in test_dataloader3:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD3((data)).detach()\n",
    "    outputsD3.append(output)\n",
    "\n",
    "outputsD3 = torch.cat(outputsD3, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegmentation_tests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m segmentation_tests\n\u001b[1;32m----> 3\u001b[0m \u001b[43msegmentation_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputsD3\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alons\\Documents\\Career\\Omyc Data Analysis\\TFM\\TFM_LodeSTAR\\Test\\segmentation_tests.py:47\u001b[0m, in \u001b[0;36msegmentation_tests\u001b[1;34m(test_dataset, model_output)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_classes):\n\u001b[0;32m     46\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mreduced_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(mask[\u001b[38;5;241m0\u001b[39m,i]\u001b[38;5;241m.\u001b[39msqueeze(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 1 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset3, outputsD3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
