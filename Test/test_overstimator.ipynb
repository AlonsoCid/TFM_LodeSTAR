{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask overstimator comparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom dataset class to make it compatible with the DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# dataset1\n",
    "dataset = torch.load('dataset_C.pt')\n",
    "\n",
    "train_dataset_C = dataset['train']\n",
    "test_dataset_C = dataset['test']\n",
    "\n",
    "train_dataloader_C = DataLoader(train_dataset_C, batch_size=8, shuffle=True)\n",
    "test_dataloader_C = DataLoader(test_dataset_C, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset2\n",
    "dataset = torch.load('dataset_CE.pt')\n",
    "\n",
    "train_dataset_CE = dataset['train']\n",
    "test_dataset_CE = dataset['test']\n",
    "\n",
    "train_dataloader_CE = DataLoader(train_dataset_CE, batch_size=8, shuffle=True)\n",
    "test_dataloader_CE = DataLoader(test_dataset_CE, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset3\n",
    "dataset = torch.load('dataset_CES.pt')\n",
    "\n",
    "train_dataset_CES = dataset['train']\n",
    "test_dataset_CES = dataset['test']\n",
    "\n",
    "train_dataloader_CES = DataLoader(train_dataset_CES, batch_size=8, shuffle=True)\n",
    "test_dataloader_CES = DataLoader(test_dataset_CES, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset4\n",
    "test_dataset_CE_dense = torch.load('dataset_CE_dense.pt')\n",
    "\n",
    "test_dataloader_CE_dense = DataLoader(test_dataset_CE_dense, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigmask-mask method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        # else:\n",
    "        #     x, _, _ = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "        weights =  mask_gumbel\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        #pixel_sum_loss = 1-torch-sum(mask_gumbel)\n",
    "        current_ratio = torch.sum(mask_gumbel) / mask_gumbel.numel() # This normalize the value avaoiding of the loss, otherwise it will be too high\n",
    "        pixel_sum_loss = .01*(1 - current_ratio)\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "            \"pixel_sum_loss\": pixel_sum_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "        \n",
    "        detections = [row[::-1] for row in detections]\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85b6f92641945548b660a5ee1653207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "import numpy as np\n",
    "\n",
    "lodestarB = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "trainer_lodestarB = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarB.fit(lodestarB, train_dataloader_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3402470a0e7b48b58290c6e30a806004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lodestarB2 = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestarB2 = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarB2.fit(lodestarB2, train_dataloader_CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation tests\n",
    "Calculate the jaccard index and percentage of successful detected objects by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsB = []\n",
    "for batch in test_dataloader_C:\n",
    "    data, *_ = batch\n",
    "    output = lodestarB((data)).detach()\n",
    "    outputsB.append(output)\n",
    "\n",
    "outputsB = torch.cat(outputsB, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.7561\n",
      "Percentage of matches in class 1: 100%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_C, outputsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsB2 = []\n",
    "for batch in test_dataloader_CE:\n",
    "    data, *_ = batch\n",
    "    output = lodestarB2((data)).detach()\n",
    "    outputsB2.append(output)\n",
    "\n",
    "outputsB2 = torch.cat(outputsB2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.8628\n",
      "Percentage of matches in class 1: 92%\n",
      "Jaccard Index for class 2: 0.8295\n",
      "Percentage of matches in class 2: 98%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_CE, outputsB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For negative detection, check for a pixel and pixels right next to each other, in a way that we are able to identify individual chunks of 1, if those chunks don't overlap with the ground truth mask it's a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        # else:\n",
    "        #     x, _, _ = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "        dropout = nn.Dropout2d(p=0.1)\n",
    "        mask_gumbel = dropout(mask_gumbel)\n",
    "        \n",
    "        weights =  mask_gumbel\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "        \n",
    "        detections = [row[::-1] for row in detections]\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\utilities\\model_summary\\model_summary.py:454: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d908820577fa4a58a8c9ff7776406001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "trainer_lodestarD = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD.fit(lodestarD, train_dataloader_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26be29fa8ded47f48dd713ebc9bc399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alons\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD2 = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestarD2 = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD2.fit(lodestarD2, train_dataloader_CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation tests\n",
    "Calculate the jaccard index and percentage of successful detected objects by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD = []\n",
    "for batch in test_dataloader_C:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD((data)).detach()\n",
    "    outputsD.append(output)\n",
    "\n",
    "outputsD = torch.cat(outputsD, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.6713\n",
      "Recall %: 100%\n",
      "Average number of artifacts: 0.85\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_C, outputsD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD2 = []\n",
    "for batch in test_dataloader_CE:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD2((data)).detach()\n",
    "    outputsD2.append(output)\n",
    "\n",
    "outputsD2 = torch.cat(outputsD2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.8750\n",
      "Recall %: 89%\n",
      "Average number of artifacts: 0.25\n",
      "Class 2:\n",
      "Jaccard Index: 0.7943\n",
      "Recall %: 94%\n",
      "Average number of artifacts: 0.05\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_CE, outputsD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, *_ = zip(*test_dataset_CE)\n",
    "images = torch.stack(images)\n",
    "results_CE = lodestarD2.detect(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.60874  34.506012]\n",
      "tensor([69.5348, 51.7730])\n",
      "Type of first sample, first class: <class 'torch.Tensor'>\n",
      "Value of first sample, first class: tensor([67.6104, 21.9561])\n",
      "Sample 0 shape: torch.Size([2, 2])\n",
      "Sample 1 shape: torch.Size([2, 2])\n",
      "Sample 2 shape: torch.Size([2, 2])\n",
      "Sample 3 shape: torch.Size([2, 2])\n",
      "Sample 4 shape: torch.Size([2, 2])\n",
      "Sample 5 shape: torch.Size([2, 2])\n",
      "Sample 6 shape: torch.Size([1, 2])\n",
      "Sample 7 shape: torch.Size([2, 2])\n",
      "Sample 8 shape: torch.Size([2, 2])\n",
      "Sample 9 shape: torch.Size([2, 2])\n",
      "Sample 10 shape: torch.Size([2, 2])\n",
      "Sample 11 shape: torch.Size([2, 2])\n",
      "Sample 12 shape: torch.Size([2, 2])\n",
      "Sample 13 shape: torch.Size([2, 2])\n",
      "Sample 14 shape: torch.Size([2, 2])\n",
      "Sample 15 shape: torch.Size([2, 2])\n",
      "Sample 16 shape: torch.Size([2, 2])\n",
      "Sample 17 shape: torch.Size([2, 2])\n",
      "Sample 18 shape: torch.Size([2, 2])\n",
      "Sample 19 shape: torch.Size([2, 2])\n",
      "Sample 20 shape: torch.Size([2, 2])\n",
      "Sample 21 shape: torch.Size([1, 2])\n",
      "Sample 22 shape: torch.Size([1, 2])\n",
      "Sample 23 shape: torch.Size([2, 2])\n",
      "Sample 24 shape: torch.Size([2, 2])\n",
      "Sample 25 shape: torch.Size([2, 2])\n",
      "Sample 26 shape: torch.Size([2, 2])\n",
      "Sample 27 shape: torch.Size([2, 2])\n",
      "Sample 28 shape: torch.Size([2, 2])\n",
      "Sample 29 shape: torch.Size([2, 2])\n",
      "Sample 30 shape: torch.Size([2, 2])\n",
      "Sample 31 shape: torch.Size([2, 2])\n",
      "Sample 32 shape: torch.Size([2, 2])\n",
      "Sample 33 shape: torch.Size([2, 2])\n",
      "Sample 34 shape: torch.Size([2, 2])\n",
      "Sample 35 shape: torch.Size([2, 2])\n",
      "Sample 36 shape: torch.Size([2, 2])\n",
      "Sample 37 shape: torch.Size([2, 2])\n",
      "Sample 38 shape: torch.Size([2, 2])\n",
      "Sample 39 shape: torch.Size([2, 2])\n",
      "Sample 40 shape: torch.Size([2, 2])\n",
      "Sample 41 shape: torch.Size([2, 2])\n",
      "Sample 42 shape: torch.Size([2, 2])\n",
      "Sample 43 shape: torch.Size([2, 2])\n",
      "Sample 44 shape: torch.Size([2, 2])\n",
      "Sample 45 shape: torch.Size([1, 2])\n",
      "Sample 46 shape: torch.Size([2, 2])\n",
      "Sample 47 shape: torch.Size([1, 2])\n",
      "Sample 48 shape: torch.Size([2, 2])\n",
      "Sample 49 shape: torch.Size([2, 2])\n",
      "Sample 50 shape: torch.Size([2, 2])\n",
      "Sample 51 shape: torch.Size([2, 2])\n",
      "Sample 52 shape: torch.Size([2, 2])\n",
      "Sample 53 shape: torch.Size([2, 2])\n",
      "Sample 54 shape: torch.Size([2, 2])\n",
      "Sample 55 shape: torch.Size([2, 2])\n",
      "Sample 56 shape: torch.Size([2, 2])\n",
      "Sample 57 shape: torch.Size([2, 2])\n",
      "Sample 58 shape: torch.Size([2, 2])\n",
      "Sample 59 shape: torch.Size([2, 2])\n",
      "Sample 60 shape: torch.Size([2, 2])\n",
      "Sample 61 shape: torch.Size([2, 2])\n",
      "Sample 62 shape: torch.Size([2, 2])\n",
      "Sample 63 shape: torch.Size([2, 2])\n",
      "Sample 64 shape: torch.Size([2, 2])\n",
      "Sample 65 shape: torch.Size([2, 2])\n",
      "Sample 66 shape: torch.Size([2, 2])\n",
      "Sample 67 shape: torch.Size([1, 2])\n",
      "Sample 68 shape: torch.Size([2, 2])\n",
      "Sample 69 shape: torch.Size([2, 2])\n",
      "Sample 70 shape: torch.Size([2, 2])\n",
      "Sample 71 shape: torch.Size([3, 2])\n",
      "Sample 72 shape: torch.Size([2, 2])\n",
      "Sample 73 shape: torch.Size([2, 2])\n",
      "Sample 74 shape: torch.Size([2, 2])\n",
      "Sample 75 shape: torch.Size([2, 2])\n",
      "Sample 76 shape: torch.Size([2, 2])\n",
      "Sample 77 shape: torch.Size([2, 2])\n",
      "Sample 78 shape: torch.Size([2, 2])\n",
      "Sample 79 shape: torch.Size([1, 2])\n",
      "Sample 80 shape: torch.Size([2, 2])\n",
      "Sample 81 shape: torch.Size([2, 2])\n",
      "Sample 82 shape: torch.Size([2, 2])\n",
      "Sample 83 shape: torch.Size([2, 2])\n",
      "Sample 84 shape: torch.Size([1, 2])\n",
      "Sample 85 shape: torch.Size([2, 2])\n",
      "Sample 86 shape: torch.Size([2, 2])\n",
      "Sample 87 shape: torch.Size([2, 2])\n",
      "Sample 88 shape: torch.Size([2, 2])\n",
      "Sample 89 shape: torch.Size([2, 2])\n",
      "Sample 90 shape: torch.Size([2, 2])\n",
      "Sample 91 shape: torch.Size([2, 2])\n",
      "Sample 92 shape: torch.Size([2, 2])\n",
      "Sample 93 shape: torch.Size([1, 2])\n",
      "Sample 94 shape: torch.Size([2, 2])\n",
      "Sample 95 shape: torch.Size([2, 2])\n",
      "Sample 96 shape: torch.Size([2, 2])\n",
      "Sample 97 shape: torch.Size([1, 2])\n",
      "Sample 98 shape: torch.Size([1, 2])\n",
      "Sample 99 shape: torch.Size([2, 2])\n",
      "Shape of final padded tensor: torch.Size([100, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# def accuracy_tests(test_dataset, detect_results, n_class=1):\n",
    "#     # extract positions\n",
    "#     positions = list(zip(*test_dataset))[-n_class:] # Tis create a list with shape: positions[class][sample][x or y]\n",
    "#      # The model outputs the coordinates with shape: predictions[sample][class][x or y]\n",
    "#     print(detect_results)\n",
    "#     mse_list = []\n",
    "\n",
    "#     for class_ in range(n_class):\n",
    "#         for sample in range(len(detect_results)):\n",
    "#             x = positions[class_][sample]\n",
    "#             y = torch.from_numpy(detect_results[sample][class_])\n",
    "\n",
    "#             # Calculate Mean Squared Error (MSE) \n",
    "#             mse = torch.mean((y - x) ** 2)\n",
    "#             mse_list.append(mse.item())\n",
    "#     return mse_list\n",
    "\n",
    "# accuracy_tests(test_dataset_CE, results_CE, 2)\n",
    "\n",
    "print(results_CE[97][0])\n",
    "tensor = torch_tensor = torch.from_numpy(results_CE[7][0])\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "def get_euclidean_distance(test_dataset, detect_results, model_output):\n",
    "\n",
    "    # Get segmentation masks\n",
    "    n_classes = model_output.size(1)-5\n",
    "    mask = model_output[:, 4:n_classes+4]\n",
    "\n",
    "    # Get gt positions\n",
    "    positions = [item[-n_classes:] for item in test_dataset]\n",
    "    positions = [torch.stack(item) for item in positions]\n",
    "    positions = torch.stack(positions)\n",
    "    positions = positions / 2\n",
    "\n",
    "    # x, y = positions[0][0]\n",
    "    # print(positions[0][0])\n",
    "    print(mask.shape)\n",
    "    \n",
    "    # Create a tensor to store the distances\n",
    "    n_samples = len(positions)\n",
    "    n_classes = n_classes\n",
    "    distances = torch.zeros((n_samples, n_classes, 1))\n",
    "\n",
    "    # Make calculations\n",
    "    for sample in range(n_samples):\n",
    "        for class_ in range(n_classes):\n",
    "            x = positions[sample][class_]\n",
    "\n",
    "            # Check if all positions in the image have been predicted, if the aren't skip the sample\n",
    "            if class_ >= len(detect_results[sample]):\n",
    "                continue\n",
    "            \n",
    "            y = detect_results[sample][class_]\n",
    "\n",
    "            # # Check that tensor y is not empty\n",
    "            # if len(y) == 0:\n",
    "            #     continue\n",
    "            y = torch.from_numpy(y)\n",
    "            \n",
    "            # Check that the class has been detected \n",
    "            a, b = x\n",
    "            if mask[sample][class_][int(a), int(b)] != 1:\n",
    "                continue\n",
    "\n",
    "            distance = torch.sqrt(torch.sum((x - y)**2))\n",
    "            distances[sample][class_] = distance\n",
    "    return distances\n",
    "\n",
    "\n",
    "distances = get_euclidean_distance(test_dataset_CE, results_CE, outputsD2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
