{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask overstimator comparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom dataset class to make it compatible with the DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# dataset1\n",
    "dataset = torch.load('dataset_C.pt')\n",
    "\n",
    "train_dataset_C = dataset['train']\n",
    "test_dataset_C = dataset['test']\n",
    "\n",
    "train_dataloader_C = DataLoader(train_dataset_C, batch_size=8, shuffle=True)\n",
    "test_dataloader_C = DataLoader(test_dataset_C, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset2\n",
    "dataset = torch.load('dataset_CE.pt')\n",
    "\n",
    "train_dataset_CE = dataset['train']\n",
    "test_dataset_CE = dataset['test']\n",
    "\n",
    "train_dataloader_CE = DataLoader(train_dataset_CE, batch_size=8, shuffle=True)\n",
    "test_dataloader_CE = DataLoader(test_dataset_CE, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset3\n",
    "dataset = torch.load('dataset_CES.pt')\n",
    "\n",
    "train_dataset_CES = dataset['train']\n",
    "test_dataset_CES = dataset['test']\n",
    "\n",
    "train_dataloader_CES = DataLoader(train_dataset_CES, batch_size=8, shuffle=True)\n",
    "test_dataloader_CES = DataLoader(test_dataset_CES, batch_size=8, shuffle=False)\n",
    "\n",
    "# dataset4\n",
    "test_dataset_CE_dense = torch.load('dataset_CE_dense.pt')\n",
    "\n",
    "test_dataloader_CE_dense = DataLoader(test_dataset_CE_dense, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigmask-mask method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        # else:\n",
    "        #     x, _, _ = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "        weights =  mask_gumbel\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        #pixel_sum_loss = 1-torch-sum(mask_gumbel)\n",
    "        current_ratio = torch.sum(mask_gumbel) / mask_gumbel.numel() # This normalize the value avaoiding of the loss, otherwise it will be too high\n",
    "        pixel_sum_loss = .01*(1 - current_ratio)\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "            \"pixel_sum_loss\": pixel_sum_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "        \n",
    "        detections = [row[::-1] for row in detections]\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85b6f92641945548b660a5ee1653207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "import numpy as np\n",
    "\n",
    "lodestarB = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "trainer_lodestarB = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarB.fit(lodestarB, train_dataloader_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3402470a0e7b48b58290c6e30a806004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lodestarB2 = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestarB2 = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarB2.fit(lodestarB2, train_dataloader_CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation tests\n",
    "Calculate the jaccard index and percentage of successful detected objects by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsB = []\n",
    "for batch in test_dataloader_C:\n",
    "    data, *_ = batch\n",
    "    output = lodestarB((data)).detach()\n",
    "    outputsB.append(output)\n",
    "\n",
    "outputsB = torch.cat(outputsB, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.7561\n",
      "Percentage of matches in class 1: 100%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_C, outputsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsB2 = []\n",
    "for batch in test_dataloader_CE:\n",
    "    data, *_ = batch\n",
    "    output = lodestarB2((data)).detach()\n",
    "    outputsB2.append(output)\n",
    "\n",
    "outputsB2 = torch.cat(outputsB2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.8628\n",
      "Percentage of matches in class 1: 92%\n",
      "Jaccard Index for class 2: 0.8295\n",
      "Percentage of matches in class 2: 98%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_CE, outputsB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For negative detection, check for a pixel and pixels right next to each other, in a way that we are able to identify individual chunks of 1, if those chunks don't overlap with the ground truth mask it's a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        # else:\n",
    "        #     x, _, _ = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:]\n",
    "\n",
    "        dropout = nn.Dropout2d(p=0.1)\n",
    "        mask_gumbel = dropout(mask_gumbel)\n",
    "        \n",
    "        weights =  mask_gumbel\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, mask_gumbel = y[:, :2], y[:, 3:4]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], mask_gumbel[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "        \n",
    "        detections = [row[::-1] for row in detections]\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf3d56013a745359e4eda0feaf66988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=1).build()\n",
    "trainer_lodestarD = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD.fit(lodestarD, train_dataloader_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b59130f0b24841b41a0b2a61454c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "\n",
    "lodestarD2 = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestarD2 = dl.Trainer(max_epochs=100, accelerator='cpu')\n",
    "trainer_lodestarD2.fit(lodestarD2, train_dataloader_CE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation tests\n",
    "Calculate the jaccard index and percentage of successful detected objects by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD = []\n",
    "for batch in test_dataloader_C:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD((data)).detach()\n",
    "    outputsD.append(output)\n",
    "\n",
    "outputsD = torch.cat(outputsD, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.7508\n",
      "Percentage of matches in class 1: 100%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_C, outputsD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputsD2 = []\n",
    "for batch in test_dataloader_CE:\n",
    "    data, *_ = batch\n",
    "    output = lodestarD2((data)).detach()\n",
    "    outputsD2.append(output)\n",
    "\n",
    "outputsD2 = torch.cat(outputsD2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Index for class 1: 0.9268\n",
      "Percentage of matches in class 1: 91%\n",
      "Jaccard Index for class 2: 0.8202\n",
      "Percentage of matches in class 2: 99%\n"
     ]
    }
   ],
   "source": [
    "from segmentation_tests import segmentation_tests\n",
    "\n",
    "segmentation_tests(test_dataset_CE, outputsD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.7508\n",
      "Recall %: 100%\n",
      "Average number of artifacts: 0.48\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "Jaccard Index: 0.9268\n",
      "Recall %: 85%\n",
      "Average number of artifacts: 1.09\n",
      "Class 2:\n",
      "Jaccard Index: 0.8202\n",
      "Recall %: 98%\n",
      "Average number of artifacts: 0.01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "def segmentation_tests(test_dataset, model_output):\n",
    "    def jaccard_index(target, pred, smooth=1e-10):\n",
    "        intersection = (pred.int() & target.int()).sum((1))\n",
    "        union = (pred.int() | target.int()).sum((1))\n",
    "        iou = (intersection + smooth) / (union + smooth)\n",
    "        return iou.mean()\n",
    "\n",
    "    # def recall(target, pred): # This one check for ground truth and mask overlaping, the new one uses positions which is more accurate and we need it for multiple object detection\n",
    "    #     intersection = (target == 1) & (pred == 1)\n",
    "    #     matches = (intersection.sum(dim=(1, 2)) > 0).sum().item()\n",
    "    #     percentage_matches = (matches / target.size(0)) * 100\n",
    "    #     return percentage_matches\n",
    "\n",
    "    def recall_and_false_positives(mask, position):\n",
    "        true_positives = 0\n",
    "        artifacts = 0\n",
    "    \n",
    "        for i, single_mask in enumerate(mask):\n",
    "            x, y = position[i]\n",
    "\n",
    "            # Check for true positive\n",
    "            if single_mask[int(x), int(y)] == 1:\n",
    "                true_positives += 1\n",
    "        \n",
    "            # Count false positives\n",
    "            labeled_array, num_regions = ndimage.label(single_mask)\n",
    "            artifacts += num_regions\n",
    "    \n",
    "        recall = (true_positives / len(mask)) * 100\n",
    "        false_positives = (artifacts - true_positives)/len(mask)\n",
    "    \n",
    "        return recall, false_positives\n",
    "    \n",
    "    # Get n_classes\n",
    "    n_classes = model_output.size(1)-5\n",
    "\n",
    "    # Get prediction mask\n",
    "    mask = model_output[:, 4:n_classes+4]\n",
    "\n",
    "    # Get original images\n",
    "    sim =[]\n",
    "    for i in range(n_classes):\n",
    "        s = [item[i+1] for item in test_dataset] # By using item[0] we could get the ground truth directly from any image\n",
    "        sim.append(torch.stack(s))\n",
    "    sim = torch.stack(sim, dim=1)\n",
    "    sim = torch.squeeze(sim, dim=2)\n",
    "\n",
    "    # Get the positions\n",
    "    positions = [item[-n_classes:] for item in test_dataset]\n",
    "    positions = [torch.stack(item) for item in positions]\n",
    "    positions = torch.stack(positions)\n",
    "    positions = positions / 2\n",
    "\n",
    "    # Downsize the image to 48x48\n",
    "    reduced_images = F.interpolate(sim, size=(48, 48), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Binarize tensors\n",
    "    reduced_images = (reduced_images[:] > 0.1).float()\n",
    "\n",
    "    # Plot the overlapping images\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(n_classes):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(reduced_images[0,i].squeeze(), cmap=\"gray\", origin=\"lower\")\n",
    "        plt.imshow(mask[0,i].squeeze(), cmap=\"jet\", alpha=0.5, origin=\"lower\")\n",
    "        plt.scatter(positions[0][i][1], positions[0][i][0], c=\"r\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        print(f\"Class {i+1}:\")\n",
    "        recall_score, fp_count = recall_and_false_positives(mask[:,i], positions[:,i])\n",
    "        print(f\"Jaccard Index: {jaccard_index(reduced_images[:, i], mask[:, i]):.4f}\")\n",
    "        print(f\"Recall %: {recall_score:.0f}%\")\n",
    "        print(f\"Average number of artifacts: {fp_count}\")\n",
    "\n",
    "segmentation_tests(test_dataset_C, outputsD)\n",
    "segmentation_tests(test_dataset_CE, outputsD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, *_ = zip(*test_dataset_CE)\n",
    "images = torch.stack(images)\n",
    "results_CE = lodestarD2.detect(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m             mse_list\u001b[38;5;241m.\u001b[39mappend(mse\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mse_list\n\u001b[1;32m---> 18\u001b[0m \u001b[43maccuracy_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset_CE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_CE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[164], line 11\u001b[0m, in \u001b[0;36maccuracy_tests\u001b[1;34m(test_dataset, detect_results, n_class)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(detect_results)):\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m positions[class_][sample]\n\u001b[1;32m---> 11\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mdetect_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Calculate Mean Squared Error (MSE) \u001b[39;00m\n\u001b[0;32m     14\u001b[0m     mse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((y \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "def accuracy_tests(test_dataset, detect_results, n_class=1):\n",
    "    # extract positions\n",
    "    positions = list(zip(*test_dataset))[-n_class:] # Tis create a list with shape: positions[class][sample][x or y]\n",
    "     # The model outputs the coordinates with shape: predictions[sample][class][x or y]\n",
    "\n",
    "    mse_list = []\n",
    "\n",
    "    for class_ in range(n_class):\n",
    "        for sample in range(len(detect_results)):\n",
    "            x = positions[class_][sample]\n",
    "            y = torch.from_numpy(detect_results[sample][class_])\n",
    "\n",
    "            # Calculate Mean Squared Error (MSE) \n",
    "            mse = torch.mean((y - x) ** 2)\n",
    "            mse_list.append(mse.item())\n",
    "    return mse_list\n",
    "\n",
    "accuracy_tests(test_dataset_CE, results_CE, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
