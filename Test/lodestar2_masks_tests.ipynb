{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a custom dataset class to make it compatible with the DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Load the data from the file\n",
    "dataset = torch.load('dataset2.pt')\n",
    "\n",
    "# Extract the individual datasets\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LodeSTAR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from skimage import morphology\n",
    "\n",
    "from deeplay import ConvolutionalNeuralNetwork, Application\n",
    "\n",
    "from deeplay.applications.detection.lodestar.transforms import (\n",
    "    RandomRotation2d,\n",
    "    RandomTranslation2d,\n",
    "    Transforms,\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LodeSTAR(Application):\n",
    "\n",
    "    # num_outputs: int # only 2D for now\n",
    "    num_classes: int\n",
    "    transforms: Transforms\n",
    "    n_transforms: int\n",
    "    model: nn.Module\n",
    "    between_loss: Callable\n",
    "    within_loss: Callable\n",
    "    between_loss_weight: float\n",
    "    within_loss_weight: float\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[nn.Module] = None,\n",
    "        # num_outputs: int = 2,\n",
    "        num_classes: int = 2,\n",
    "        transforms: Optional[Transforms] = None,\n",
    "        n_transforms: int = 2,\n",
    "        between_loss: Optional[Callable] = None,\n",
    "        within_loss: Optional[Callable] = None,\n",
    "        between_loss_weight: float = 1,\n",
    "        within_loss_weight: float = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if transforms is None:\n",
    "            transforms = Transforms(\n",
    "                [\n",
    "                    RandomTranslation2d(),\n",
    "                    RandomRotation2d(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "        self.n_transforms = n_transforms\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.between_loss = between_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.within_loss = within_loss or nn.L1Loss(reduction=\"mean\")\n",
    "        self.between_loss_weight = between_loss_weight\n",
    "        self.within_loss_weight = within_loss_weight\n",
    "\n",
    "        super().__init__(loss=None, **kwargs)\n",
    "\n",
    "    def _get_default_model(self):\n",
    "        cnn = ConvolutionalNeuralNetwork(\n",
    "            None,\n",
    "            [32, 32, 64, 64, 64, 64, 64, 64, 64],\n",
    "            (2 + 1) + (self.num_classes + 1),  # (num_outputs + 1) + (num_classes +1)\n",
    "        )\n",
    "        cnn.blocks[2].pooled()\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def transform_data(self, batch):\n",
    "        repeated = batch.repeat_interleave(self.n_transforms, dim=0)\n",
    "        transformed, inverse = self.transforms(repeated)\n",
    "        return transformed, inverse\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.training:\n",
    "            x, class_label = x\n",
    "\n",
    "        else:\n",
    "            x, _, _ = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        y = out[:, :3, ...]\n",
    "        classes=out[:, 3:, ...]\n",
    "        classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "        batch_size = classes.size(0)\n",
    "        num_channels = classes.size(1)\n",
    "        _, _, Hx, Wx = x.shape\n",
    "        _, _, Hy, Wy = y.shape\n",
    "        x_range = torch.arange(Hy, device=x.device) * Hx / Hy\n",
    "        y_range = torch.arange(Wy, device=x.device) * Wx / Wy\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.training:\n",
    "            x_range = x_range - Hx / 2 + 0.5\n",
    "            y_range = y_range - Wx / 2 + 0.5\n",
    "\n",
    "            batch_indices = torch.arange(batch_size)\n",
    "            \n",
    "            # mk = torch.ones((batch_size, num_channels), dtype=torch.bool)\n",
    "            # mk[batch_indices, class_label.squeeze()] = False\n",
    "            # mk[:, -1] = False  # Set the last channel to False for all batches\n",
    "\n",
    "            # mk = mk.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, Hy, Wy)\n",
    "            # classes[mk] *= 0.0\n",
    "            # classes = nn.functional.gumbel_softmax(classes, hard=True, dim=1)\n",
    "\n",
    "            mask = classes[batch_indices,  class_label.squeeze(), :, :][:,None]\n",
    "        else:\n",
    "            \n",
    "            mask = classes.sum(dim=1)[:,None]#torch.ones_like(y[:, 2:3, ...])\n",
    "\n",
    "\n",
    "        Y, X = torch.meshgrid(y_range, x_range, indexing=\"xy\")\n",
    "\n",
    "        delta_x = y[:, 0:1, ...]\n",
    "        delta_y = y[:, 1:2, ...]\n",
    "        yy = y[:, 2:3, ...]\n",
    "        weights = y[:, 2:3, ...].sigmoid()\n",
    "        X = X + delta_x\n",
    "        Y = Y + delta_y\n",
    "        \n",
    "        return torch.cat(\n",
    "            [X, Y, weights, mask, classes], dim=1\n",
    "        )\n",
    "\n",
    "    def normalize(self, weights):\n",
    "        weights = weights + 1e-5\n",
    "        return weights / weights.sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "    def reduce(self, X, weights):\n",
    "        return (X * weights).sum(dim=(2, 3)) / weights.sum(dim=(2, 3))\n",
    "\n",
    "    def compute_loss(self, y_hat, inverse_fn):\n",
    "        B = y_hat.size(0) / self.n_transforms\n",
    "\n",
    "        y_pred, weights, mask_gumbel, classes = y_hat[:, :2], y_hat[:, 2:3], y_hat[:, 3:4], y_hat[:, 4:] # Changed\n",
    "\n",
    "        weights =  mask_gumbel #\n",
    "        weights = self.normalize(weights)\n",
    "        y_reduced = self.reduce(y_pred, weights)\n",
    "\n",
    "        within_disagreement = (y_pred - y_reduced[..., None, None]) * weights \n",
    "        within_disagreement_loss = self.within_loss(\n",
    "            within_disagreement, torch.zeros_like(within_disagreement)\n",
    "        )\n",
    "\n",
    "        y_reduced_on_initial = inverse_fn(y_reduced)\n",
    "\n",
    "        between_disagreement_loss = 0\n",
    "\n",
    "        for i in range(0, y_pred.size(0), self.n_transforms):\n",
    "            batch_preds = y_reduced_on_initial[i : i + self.n_transforms]\n",
    "            batch_mean_pred = batch_preds.mean(dim=0, keepdim=True).expand_as(\n",
    "                batch_preds\n",
    "            )\n",
    "            between_disagreement_loss += (\n",
    "                self.between_loss(batch_preds, batch_mean_pred) / B\n",
    "            )\n",
    "        weighted_between_loss = between_disagreement_loss * self.between_loss_weight\n",
    "        weighted_within_loss = within_disagreement_loss * self.within_loss_weight\n",
    "        \n",
    "        compl_mask=classes[:,:-1,...].sum(dim=1)[:,None]-mask_gumbel\n",
    "        mask_loss = 10*compl_mask.mean(dim=(2, 3)).mean()# #- mask_gumbel.mean()\n",
    "\n",
    "        # Overestimation loss\n",
    "        kernel = torch.ones((1, 1, 3, 3), device=y_hat.device)\n",
    "        dilated_mask = F.conv2d(mask_gumbel, kernel, padding=1) > 0\n",
    "        overestimation_loss = 0.5*(dilated_mask.float() - mask_gumbel).mean()\n",
    "\n",
    "        return {\n",
    "            \"between_image_disagreement\": weighted_between_loss,\n",
    "            \"within_image_disagreement\": weighted_within_loss,\n",
    "            \"mask_loss\": mask_loss,\n",
    "            \"overestimation_loss\": overestimation_loss,\n",
    "        }\n",
    "\n",
    "    def detect(self, x, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"):\n",
    "        \"\"\"Detects objects in a batch of images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights = y_hat[:, :2], y_hat[:, 2:3]\n",
    "        detections = [\n",
    "            self.detect_single(y_pred[i], weights[i], alpha, beta, cutoff, mode)\n",
    "            for i in range(len(y_pred))\n",
    "        ]\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def pooled(self, x, mask=1):\n",
    "        \"\"\"Pooled output from model.\n",
    "\n",
    "        Predict and pool the output from the model. Useful to acquire a single output from the model.\n",
    "        Masking is supported by setting the mask to 0 where the output should be ignored.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like\n",
    "            Input to model\n",
    "        mask : array-like\n",
    "            Mask for pooling. Should be the same shape as the output from the model with a single channel.\n",
    "        \"\"\"\n",
    "        y = self(x.to(self.device))\n",
    "        y_pred, weights,  = y[:, :2], y[:, 2:3]\n",
    "        masked_weights = weights * mask\n",
    "\n",
    "        pooled = self.reduce(y_pred, self.normalize(masked_weights))\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def detect_single(\n",
    "        self, y_pred, weights, alpha=0.5, beta=0.5, cutoff=0.97, mode=\"quantile\"\n",
    "    ):\n",
    "        \"\"\"Detects objects in a single image\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        cutoff: float\n",
    "            Threshold for detection\n",
    "        mode: string\n",
    "            Mode for thresholding. Can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "        \"\"\"\n",
    "        score = self.get_detection_score(y_pred, weights, alpha, beta)\n",
    "        return self.find_local_maxima(y_pred, score, cutoff, mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_local_maxima(pred, score, cutoff=0.9, mode=\"quantile\"):\n",
    "        \"\"\"Finds the local maxima in a score-map, indicating detections\n",
    "\n",
    "        Parameters\n",
    "            ----------\n",
    "        pred, score: array-like\n",
    "            Output from model, score-map\n",
    "        cutoff, mode: float, string\n",
    "            Treshholding parameters. Mode can be either \"quantile\" or \"ratio\" or \"constant\". If \"quantile\", then\n",
    "            `ratio` defines the quantile of scores to accept. If \"ratio\", then cutoff defines the ratio of the max\n",
    "            score as threshhold. If constant, the cutoff is used directly as treshhold.\n",
    "\n",
    "        \"\"\"\n",
    "        score = score[3:-3, 3:-3]\n",
    "        th = cutoff\n",
    "        if mode == \"quantile\":\n",
    "            th = np.quantile(score, cutoff)\n",
    "        elif mode == \"ratio\":\n",
    "            th = np.max(score.flatten()) * cutoff\n",
    "        hmax = morphology.h_maxima(np.squeeze(score), th) == 1\n",
    "        hmax = np.pad(hmax, ((3, 3), (3, 3)))\n",
    "        detections = pred.permute(1, 2, 0).detach().cpu().numpy()[hmax, :]\n",
    "        return np.array(detections)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_consistency(pred):\n",
    "        \"\"\"Calculate the consistency metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred : array-like\n",
    "            first output from model\n",
    "        \"\"\"\n",
    "        pred = pred.permute(1, 2, 0).cpu().detach().numpy()\n",
    "        kernel = np.ones((3, 3, 1)) / 3**2\n",
    "        pred_local_squared = scipy.signal.convolve(pred, kernel, \"same\") ** 2\n",
    "        squared_pred_local = scipy.signal.convolve(pred**2, kernel, \"same\")\n",
    "        squared_diff = (squared_pred_local - pred_local_squared).sum(-1)\n",
    "        np.clip(squared_diff, 0, np.inf, squared_diff)\n",
    "        return 1 / (1e-6 + squared_diff)\n",
    "\n",
    "    @classmethod\n",
    "    def get_detection_score(cls, pred, weights, alpha=0.5, beta=0.5):\n",
    "        \"\"\"Calculates the detection score as weights^alpha * consistency^beta.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pred, weights: array-like\n",
    "            Output from model\n",
    "        alpha, beta: float\n",
    "            Geometric weight of the weight-map vs the consistenct metric for detection.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            weights[0].detach().cpu().numpy() ** alpha\n",
    "            * cls.local_consistency(pred) ** beta\n",
    "        )\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        batch, class_label = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        class_label = class_label.repeat_interleave(self.n_transforms, dim=0) # This makes to match class_labels with the augmented data\n",
    "        return (x, class_label), inverse\n",
    "\n",
    "    def val_preprocess(self, batch):\n",
    "        batch,_,_ = batch\n",
    "        x, inverse = self.transform_data(batch)\n",
    "        return (x,), inverse\n",
    "\n",
    "    test_preprocess = val_preprocess\n",
    "\n",
    "    def on_train_end(self) -> None:\n",
    "        self.eval()\n",
    "        return super().on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplay as dl\n",
    "import numpy as np\n",
    "\n",
    "lodestar = LodeSTAR(optimizer=dl.Adam(lr=1e-4), num_classes=2).build()\n",
    "trainer_lodestar = dl.Trainer(max_epochs=100, accelerator='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ between_loss  │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ within_loss   │ L1Loss                     │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ train_metrics │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ val_metrics   │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ test_metrics  │ MetricCollection           │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ConvolutionalNeuralNetwork │  252 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ between_loss  │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ within_loss   │ L1Loss                     │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection           │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ optimizer     │ Adam                       │      0 │\n",
       "└───┴───────────────┴────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 252 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 252 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 252 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 252 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be58862582d471da7f8656e9ef0c291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_lodestar.fit(lodestar, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "for batch in test_dataloader:\n",
    "    data, part1, part2, pos1, pos2 = batch\n",
    "    output = lodestar((data, pos1, pos2)).detach()\n",
    "    all_outputs.append(output)\n",
    "\n",
    "all_outputs = torch.cat(all_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(93.6512)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1,1,...].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation mask out of the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "eval_masks = all_outputs[:, 4:7,...]\n",
    "print(eval_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the dataset object to a list\n",
    "all_samples = [test_dataset[i] for i in range(len(test_dataset))]\n",
    "\n",
    "# Extract and concatenate simulations\n",
    "selected_data = []\n",
    "for sample in all_samples:\n",
    "    selected_data.append(torch.stack([sample[1].squeeze(), sample[2].squeeze(), sample[0].squeeze()]))\n",
    "\n",
    "# Convert list to tensor\n",
    "selected_data = torch.stack(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Downsize the image to 48x48\n",
    "gt_mask = F.interpolate(selected_data, size=(48, 48), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Binarize tensors\n",
    "gt_mask[:, 0, ...] = (gt_mask[:, 0, ...] > 0.1).float()\n",
    "gt_mask[:, 1, ...] = (gt_mask[:, 1, ...] > 0.1).float()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(2):\n",
    "\tplt.subplot(2, 4, i + 1)\n",
    "\tplt.imshow(gt_mask[70, i,...].squeeze(), cmap=\"gray\", origin=\"lower\")\n",
    "\tplt.imshow(eval_masks[70, i,...].squeeze(), cmap=\"jet\", alpha=0.5, origin=\"lower\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Jaccard index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8389)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_index(pred, target, smooth=1e-10):\n",
    "    intersection = (pred.int() & target.int()).sum((1, 2))\n",
    "    union = (pred.int() | target.int()).sum((1, 2))\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.mean()\n",
    "\n",
    "jaccard_index(eval_masks[:, :2,...], gt_mask[:, :2, ...], smooth=1e-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplay_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
